{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access here for colab with the code which is already runned:https://colab.research.google.com/drive/1-4uDO4KOJHpKUAD3d21HS3iJGw1B7VXn#scrollTo=NsVi7wyH0wRK\n",
    "\n",
    "\n",
    "#Data Preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CIFAR-10 dataset (replace with your custom dataset if needed)\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the data to [0, 1] range\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels (10 classes)\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Class names for CIFAR-10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Function to plot sample images\n",
    "def plot_images(images, labels, class_names):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    batch_size = len(images)  # Dynamically adjust based on batch size\n",
    "    rows = int(np.sqrt(batch_size))\n",
    "    cols = int(np.ceil(batch_size / rows))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i])\n",
    "        plt.xlabel(class_names[np.argmax(labels[i])])\n",
    "    plt.show()\n",
    "\n",
    "# Display a few sample images\n",
    "plot_images(X_train[:9], y_train[:9], class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "# Fit the data generator on the training data\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Display augmented images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "    plot_images(X_batch, y_batch, class_names)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3), padding='same', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Adam optimizer\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# No augmentation for validation data\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# Fit the data generators\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=64)\n",
    "validation_generator = test_datagen.flow(X_test, y_test, batch_size=64)\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model.keras',  # Filepath where the model is saved\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    mode='min',  # Save the model with the minimum validation loss\n",
    "    save_best_only=True,  # Only save the model when validation loss improves\n",
    "    verbose=1  # Print information when the model is saved\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,  # Training data generator\n",
    "    epochs=150,  # Number of epochs to train for\n",
    "    validation_data=validation_generator,  # Validation data generator\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr],  # Use ModelCheckpoint, EarlyStopping, and ReduceLROnPlateau callbacks\n",
    "    verbose=2  # Display detailed logs for each epoch\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualising metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for final training and validation accuracy\n",
    "train_acc = history.history['accuracy'][-1]\n",
    "val_acc = history.history['val_accuracy'][-1]\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(['Training Accuracy', 'Validation Accuracy'], [train_acc, val_acc], color=['blue', 'green'])\n",
    "plt.title('Final Training and Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for final training and validation loss\n",
    "train_loss = history.history['loss'][-1]  # Last training loss value\n",
    "val_loss = history.history['val_loss'][-1]  # Last validation loss value\n",
    "\n",
    "# Create the bar chart\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(['Training Loss', 'Validation Loss'], [train_loss, val_loss], color=['blue', 'green'])\n",
    "plt.title('Final Training and Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)  # Add grid lines for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation +visualisation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\n",
    "y_true = np.argmax(y_test, axis=1)  # Convert true labels to class labels\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(y_true, y_pred_classes, target_names=class_names, output_dict=True)\n",
    "print(classification_report(y_true, y_pred_classes, target_names=class_names))\n",
    "\n",
    "# Extracting precision, recall, and F1-score for bar chart plotting\n",
    "precision = [report[label]['precision'] for label in class_names]\n",
    "recall = [report[label]['recall'] for label in class_names]\n",
    "f1_score = [report[label]['f1-score'] for label in class_names]\n",
    "\n",
    "# Plotting Precision, Recall, and F1-score bar charts\n",
    "x = np.arange(len(class_names))  # Label locations\n",
    "width = 0.2  # Width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width, precision, width, label='Precision', color='skyblue')\n",
    "rects2 = ax.bar(x, recall, width, label='Recall', color='lightgreen')\n",
    "rects3 = ax.bar(x + width, f1_score, width, label='F1-Score', color='salmon')\n",
    "\n",
    "# Add some text for labels, title, and custom x-axis tick labels\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Precision, Recall, and F1-Score per Class')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names)\n",
    "ax.legend()\n",
    "\n",
    "# Display the bar chart\n",
    "plt.show()\n",
    "\n",
    "# Plotting total predictions per class\n",
    "total_predictions = np.sum(conf_matrix, axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=class_names, y=total_predictions, palette='Blues_d', hue=class_names, legend=False)\n",
    "plt.title('Total Predictions per Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Predictions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VGG16 from Keras with pre-trained ImageNet weights\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "\n",
    "# Normalize the data to [0, 1] range\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "\n",
    "# Load the VGG16 model with ImageNet weights, exclude the top (fully-connected) layers\n",
    "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "\n",
    "# Freeze all layers in the VGG16 base model initially\n",
    "vgg_base.trainable = False\n",
    "\n",
    "\n",
    "# Add custom top layers for CIFAR-10 classification\n",
    "x = vgg_base.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(10, activation='softmax')(x)\n",
    "\n",
    "\n",
    "# Create a new model combining the VGG16 base and the custom layers\n",
    "model_vgg = Model(inputs=vgg_base.input, outputs=output)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model_vgg.compile(optimizer=Adam(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Define callbacks for training\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='vgg16_finetuned_best_model.keras',\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model with the frozen VGG16 layers\n",
    "history_vgg = model_vgg.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=64),\n",
    "    epochs=30,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Unfreeze the last 10 layers of VGG16 for fine-tuning\n",
    "for layer in vgg_base.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "# Recompile the model after unfreezing 10 layers\n",
    "model_vgg.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fine-tune the model again\n",
    "history_fine_tune = model_vgg.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=64),\n",
    "    epochs=40,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "# Unfreeze more layers (up to 20) for further fine-tuning\n",
    "for layer in vgg_base.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "# Recompile the model again after unfreezing more layers\n",
    "model_vgg.compile(optimizer=Adam(learning_rate=0.00005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fine-tune with 20 unfrozen layers\n",
    "history_fine_tune_more = model_vgg.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=64),\n",
    "    epochs=50,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model_vgg.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# If test accuracy is above 95%, save the final model\n",
    "if test_acc >= 0.95:\n",
    "    model_vgg.save('final_vgg16_model_95_acc.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Visualizing metrics and comparing with custom CNN\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training and validation accuracy for VGG16\n",
    "plt.plot(history_vgg.history['accuracy'], label='VGG16 Training Accuracy', linestyle='--', color='blue')\n",
    "plt.plot(history_vgg.history['val_accuracy'], label='VGG16 Validation Accuracy', linestyle='-', color='blue')\n",
    "plt.plot(history_fine_tune.history['accuracy'], label='VGG16 Fine-tune Training Accuracy', linestyle='--', color='green')\n",
    "plt.plot(history_fine_tune.history['val_accuracy'], label='VGG16 Fine-tune Validation Accuracy', linestyle='-', color='green')\n",
    "plt.plot(history_fine_tune_more.history['accuracy'], label='VGG16 Fine-tune More Training Accuracy', linestyle='--', color='red')\n",
    "plt.plot(history_fine_tune_more.history['val_accuracy'], label='VGG16 Fine-tune More Validation Accuracy', linestyle='-', color='red')\n",
    "\n",
    "plt.title('Training and Validation Accuracy for VGG16')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss for VGG16\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history_vgg.history['loss'], label='VGG16 Training Loss', linestyle='--', color='blue')\n",
    "plt.plot(history_vgg.history['val_loss'], label='VGG16 Validation Loss', linestyle='-', color='blue')\n",
    "plt.plot(history_fine_tune.history['loss'], label='VGG16 Fine-tune Training Loss', linestyle='--', color='green')\n",
    "plt.plot(history_fine_tune.history['val_loss'], label='VGG16 Fine-tune Validation Loss', linestyle='-', color='green')\n",
    "plt.plot(history_fine_tune_more.history['loss'], label='VGG16 Fine-tune More Training Loss', linestyle='--', color='red')\n",
    "plt.plot(history_fine_tune_more.history['val_loss'], label='VGG16 Fine-tune More Validation Loss', linestyle='-', color='red')\n",
    "\n",
    "plt.title('Training and Validation Loss for VGG16')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix and Classification Report for VGG16\n",
    "y_pred_vgg = model_vgg.predict(X_test)\n",
    "y_pred_classes_vgg = np.argmax(y_pred_vgg, axis=1)\n",
    "y_true_vgg = np.argmax(y_test, axis=1)\n",
    "\n",
    "conf_matrix_vgg = confusion_matrix(y_true_vgg, y_pred_classes_vgg)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_vgg, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(10), yticklabels=np.arange(10))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for VGG16 Model')\n",
    "plt.show()\n",
    "report_vgg = classification_report(y_true_vgg, y_pred_classes_vgg, output_dict=True)\n",
    "print(classification_report(y_true_vgg, y_pred_classes_vgg))\n",
    "\n",
    "# Extract precision, recall, and F1-score for bar chart plotting for VGG16\n",
    "precision_vgg = [report_vgg[str(i)]['precision'] for i in range(10)]\n",
    "recall_vgg = [report_vgg[str(i)]['recall'] for i in range(10)]\n",
    "f1_score_vgg = [report_vgg[str(i)]['f1-score'] for i in range(10)]\n",
    "\n",
    "# Plotting Precision, Recall, and F1-score bar charts\n",
    "x = np.arange(10)  # Label locations\n",
    "width = 0.2  # Width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width, precision_vgg, width, label='Precision', color='skyblue')\n",
    "rects2 = ax.bar(x, recall_vgg, width, label='Recall', color='lightgreen')\n",
    "rects3 = ax.bar(x + width, f1_score_vgg, width, label='F1-Score', color='salmon')\n",
    "\n",
    "# Add some text for labels, title, and custom x-axis tick labels\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Precision, Recall, and F1-Score per Class for VGG16')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(range(10))\n",
    "ax.legend()\n",
    "\n",
    "# Display the bar chart\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
